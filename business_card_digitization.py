# -*- coding: utf-8 -*-
"""business_card_digitization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lnSKytuPqTHXLFxYd1syuWjCXDkHhnFx
"""

# ==================== Install Dependencies ====================
!pip install flask pillow easyocr spacy opencv-python numpy openpyxl nameparser
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
!pip install pyngrok
!python -m spacy download en_core_web_trf

pip install Streamlit

# ==================== Import Libraries ====================
import gradio as gr
from PIL import Image
import easyocr
import spacy
import re
import os
import cv2
import numpy as np
from openpyxl import Workbook, load_workbook
from spacy.matcher import Matcher
from nameparser import HumanName

# ==================== Initialize OCR and NLP ====================
reader = easyocr.Reader(['en'])
nlp = spacy.load("en_core_web_trf")
matcher = Matcher(nlp.vocab)

profession_list = [
    "software engineer", "graphic designer", "data scientist",
    "founder", "ceo", "cto", "analyst", "consultant",
    "developer", "manager", "architect", "accountant",
    "marketing", "officer", "president", "administrator",
    "seo", "designer", "engineer"
]
profession_patterns = [[{"LOWER": token} for token in title.split()] for title in profession_list]
matcher.add("PROFESSION", profession_patterns)

# ==================== Helper Functions ====================
def preprocess_image(pil_image):
    img = np.array(pil_image)
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    denoised = cv2.fastNlMeansDenoising(gray, h=10)
    _, binary = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
    return binary

def clean_text(text):
    text = re.sub(r"[|_‚Ä¢]", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()

def extract_name(lines, doc):
    for ent in doc.ents:
        if ent.label_ == "PERSON" and len(ent.text.split()) >= 2:
            return ent.text.strip()
    for line in lines:
        name = HumanName(line.strip())
        if name.first and name.last:
            return str(name)
    return None

def extract_email(text):
    cleaned_text = text.replace(" ", "") \
                       .replace("(@)", "@").replace("[at]", "@").replace("{at}", "@") \
                       .replace("(dot)", ".").replace("[dot]", ".").replace("{dot}", ".")
    match = re.search(r"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}", cleaned_text)
    return match.group() if match else None

def extract_phone(text):
    phone_match = re.findall(r"(\+?\d[\d\s\-]{7,15})", text)
    if phone_match:
        phone = phone_match[0]
        phone = re.sub(r"\s+", "-", phone)
        return phone.strip("-")
    return None

def extract_address(lines, doc):
    for ent in doc.ents:
        if ent.label_ in ["GPE", "LOC", "FAC", "ORG"]:
            return ent.text.strip()
    for line in lines:
        if any(kw in line.lower() for kw in ["street", "road", "city", "state", "block", "avenue", "plaza", "phase"]):
            return line.strip()
    return None

def extract_profession(doc, lines):
    matches = matcher(doc)
    if matches:
        for match_id, start, end in matches:
            return doc[start:end].text.strip()
    for line in lines:
        if any(job.lower() in line.lower() for job in profession_list):
            return line.strip()
    return None

def extract_structured_data(text):
    text = clean_text(text)
    data = {"name": None, "email": None, "phone": None, "address": None, "profession": None}
    lines = text.split("\n")
    doc = nlp(text)

    data["name"] = extract_name(lines, doc)
    data["email"] = extract_email(text)
    data["phone"] = extract_phone(text)
    data["address"] = extract_address(lines, doc)
    data["profession"] = extract_profession(doc, lines)

    return data

def save_to_excel(dataset, filename="data.xlsx"):
    if not os.path.exists(filename):
        wb = Workbook()
        ws = wb.active
        ws.append(["Name", "Email", "Phone", "Address", "Profession"])
    else:
        wb = load_workbook(filename)
        ws = wb.active

    for data in dataset:
        ws.append([data.get("name", ""), data.get("email", ""), data.get("phone", ""),
                   data.get("address", ""), data.get("profession", "")])
    wb.save(filename)
    return filename

# ==================== Gradio App ====================
def process_cards(filepaths):
    all_texts = []
    all_data = {}

    # Ensure we always loop over a list
    if isinstance(filepaths, str):
        filepaths = [filepaths]

    for fp in filepaths:
        try:
            filename = os.path.basename(fp)
            image = Image.open(fp).convert("RGB")
            preprocessed = preprocess_image(image)

            # OCR
            results_ocr = reader.readtext(preprocessed)
            text = '\n'.join([res[1] for res in results_ocr if res[2] > 0.5])

            # Structured extraction
            structured_data = extract_structured_data(text)

            # Store results
            all_texts.append(f"**{filename}**:\n{text}")
            all_data[filename] = structured_data
        except Exception as e:
            all_texts.append(f"‚ö†Ô∏è Error processing {fp}: {e}")
            all_data[filename] = {}

    # Save all extracted data into one Excel
    excel_file = save_to_excel(all_data.values())

    return "\n\n---\n\n".join(all_texts), all_data, excel_file


with gr.Blocks() as demo:
    gr.Markdown("## üìá Business Card OCR & Digitizer (Batch Mode)\nUpload one or more business cards and extract structured details.")

    with gr.Row():
        inp = gr.File(
            type="filepath",
            file_types=[".png", ".jpg", ".jpeg"],
            label="Upload Business Cards",
            file_count="multiple"
        )
    with gr.Row():
        raw_text = gr.Markdown(label="üìú Extracted Raw Text (all cards)")
    with gr.Row():
        structured = gr.JSON(label="üìå Structured Data (per card)")
    with gr.Row():
        file_out = gr.File(label="‚¨áÔ∏è Download Excel (all cards)")

    inp.change(process_cards, inp, [raw_text, structured, file_out])

demo.launch()